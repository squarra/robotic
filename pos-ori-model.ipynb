{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37e64552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d986d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"dataset-pos-ori.h5\"\n",
    "DEVICE = torch.device(\"cuda:3\")\n",
    "BATCH_SIZE = 512\n",
    "SUBSET_FRACTION = 1.0\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3c4b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(batch: tuple[torch.Tensor, ...]):\n",
    "    return tuple(item.to(DEVICE, non_blocking=True) for item in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8ddc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_batch(depth, mask, quat):\n",
    "    batch_size = depth.shape[0]\n",
    "\n",
    "    # Depth augmentation\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        noise = torch.randn_like(depth) * 0.02\n",
    "        depth = depth + noise\n",
    "\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        scales = torch.empty(batch_size, 1, 1, device=DEVICE).uniform_(0.95, 1.05)\n",
    "        depth = depth * scales\n",
    "\n",
    "    if torch.rand(1).item() < 0.3:\n",
    "        offsets = torch.empty(batch_size, 1, 1, device=DEVICE).uniform_(-0.03, 0.03)\n",
    "        depth = depth + offsets\n",
    "\n",
    "    depth = torch.clamp(depth, 0, 1)\n",
    "\n",
    "    # Mask augmentation\n",
    "    if torch.rand(1).item() < 0.4:\n",
    "        mask_4d = mask.unsqueeze(1)\n",
    "        if torch.rand(1).item() < 0.8:\n",
    "            mask_4d = F.max_pool2d(mask_4d, 3, stride=1, padding=1)\n",
    "        else:\n",
    "            mask_4d = -F.max_pool2d(-mask_4d, 3, stride=1, padding=1)\n",
    "        mask = mask_4d.squeeze(1)\n",
    "\n",
    "    # Quaternion augmentation\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        angles = torch.empty(batch_size, device=DEVICE).uniform_(-0.175, 0.175)\n",
    "        half_angles = angles / 2\n",
    "\n",
    "        perturb_quats = torch.stack(\n",
    "            [torch.cos(half_angles), torch.zeros_like(half_angles), torch.zeros_like(half_angles), torch.sin(half_angles)], dim=1\n",
    "        )\n",
    "\n",
    "        w1, x1, y1, z1 = perturb_quats.unbind(1)\n",
    "        w2, x2, y2, z2 = quat.unbind(1)\n",
    "\n",
    "        quat = torch.stack(\n",
    "            [\n",
    "                w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2,\n",
    "                w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2,\n",
    "                w1 * y2 - x1 * z2 + y1 * w2 + z1 * x2,\n",
    "                w1 * z2 + x1 * y2 - y1 * x2 + z1 * w2,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        quat = F.normalize(quat, dim=1)\n",
    "\n",
    "    return depth, mask, quat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d902996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self._h5_file = None\n",
    "        with h5py.File(DATASET_PATH, \"r\") as f:\n",
    "            self.keys = list(f.keys())\n",
    "            self.depth_min = f.attrs[\"depth_min\"]\n",
    "            self.depth_max = f.attrs[\"depth_max\"]\n",
    "            self.dist_max = f.attrs[\"dist_max\"]\n",
    "\n",
    "    def _init_h5(self):\n",
    "        if self._h5_file is None:\n",
    "            self._h5_file = h5py.File(DATASET_PATH, \"r\")\n",
    "\n",
    "    def close(self):\n",
    "        if self._h5_file is not None:\n",
    "            self._h5_file.close()\n",
    "            self._h5_file = None\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._init_h5()\n",
    "        dp = self._h5_file[self.keys[idx]]\n",
    "        return dp[\"depth\"][()], dp[\"mask\"][()], dp[\"dist_map\"][()], dp[\"quat\"][()], dp[\"target_pose\"][()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea0f804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 49694\n",
      "Sample shapes: [(256, 384), (256, 384), (256, 384), (4,), (7,)]\n"
     ]
    }
   ],
   "source": [
    "dataset = AllDataset()\n",
    "\n",
    "indices = random.sample(range(len(dataset)), int(len(dataset) * SUBSET_FRACTION))\n",
    "dataset = torch.utils.data.Subset(dataset, indices)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Sample shapes: {[item.shape for item in dataset[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57b26fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=(256, 384), patch_size=16, in_channels=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size, img_size[1] // patch_size)\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        nn.init.trunc_normal_(self.proj.weight, std=0.02)\n",
    "        nn.init.zeros_(self.proj.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size=(256, 384), patch_size=16, in_channels=3, embed_dim=192, depth=4, num_heads=6, mlp_ratio=2.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=False,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        self._init_transformer_weights()\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def _init_transformer_weights(self):\n",
    "        for module in self.transformer.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.trunc_normal_(module.weight, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "        x = x[:, 0]\n",
    "        return x\n",
    "\n",
    "\n",
    "class AllNetViT(nn.Module):\n",
    "    def __init__(self, img_size=(256, 384), patch_size=16, embed_dim=192, depth=4, num_heads=6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vision_encoder = ViT(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=3,\n",
    "            embed_dim=embed_dim,\n",
    "            depth=depth,\n",
    "            num_heads=num_heads,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "\n",
    "        self.quat_encoder = nn.Sequential(\n",
    "            nn.Linear(4, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(embed_dim + 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 7),\n",
    "        )\n",
    "\n",
    "    def forward(self, depth, mask, goal, quat):\n",
    "        vision_feat = self.vision_encoder(torch.stack([depth, mask, goal], dim=1))\n",
    "        quat_feat = self.quat_encoder(quat)\n",
    "        return self.regressor(torch.cat([vision_feat, quat_feat], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3a9758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 851,527\n"
     ]
    }
   ],
   "source": [
    "model = AllNetViT(img_size=(256, 384), patch_size=8, embed_dim=128, depth=3, num_heads=4).to(DEVICE)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=150, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b8bf237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grad_norms():\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            if grad_norm < 1e-7 or grad_norm > 5e-1:\n",
    "                print(f\"{name:50s} | grad_norm: {grad_norm:.2e}\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def debug_model():\n",
    "    model.eval()\n",
    "    batch = next(iter(train_loader))\n",
    "    depth, mask, dist_map, quat, _ = move_to_device(batch)\n",
    "\n",
    "    vision_input = torch.stack([depth, mask, dist_map], dim=1)\n",
    "    B = vision_input.shape[0]\n",
    "\n",
    "    patches = model.vision_encoder.patch_embed(vision_input)\n",
    "    cls_tokens = model.vision_encoder.cls_token.expand(B, -1, -1)\n",
    "    patches_with_cls = torch.cat([cls_tokens, patches], dim=1)\n",
    "    patches_pos = patches_with_cls + model.vision_encoder.pos_embed\n",
    "    transformed = model.vision_encoder.transformer(patches_pos)\n",
    "    cls_output = transformed[:, 0]\n",
    "\n",
    "    quat_feat = model.quat_encoder(quat)\n",
    "    logits = model(depth, mask, dist_map, quat)\n",
    "\n",
    "    depth_impact = (logits - model(torch.zeros_like(depth), mask, dist_map, quat)).abs().mean()\n",
    "    mask_impact = (logits - model(depth, torch.zeros_like(mask), dist_map, quat)).abs().mean()\n",
    "    dist_impact = (logits - model(depth, mask, torch.zeros_like(dist_map), quat)).abs().mean()\n",
    "    quat_impact = (logits - model(depth, mask, dist_map, torch.zeros_like(quat))).abs().mean()\n",
    "\n",
    "    patch_dead = (patches.abs() < 1e-5).float().mean()\n",
    "    cls_dead = (cls_output.abs() < 1e-5).float().mean()\n",
    "    quat_dead = (quat_feat.abs() < 1e-5).float().mean()\n",
    "\n",
    "    patch_div = patches.std(dim=0).mean()\n",
    "    cls_div = cls_output.std(dim=0).mean()\n",
    "\n",
    "    print(\n",
    "        f\"impacts: d={depth_impact:.3f} m={mask_impact:.3f} \"\n",
    "        f\"dist={dist_impact:.3f} q={quat_impact:.3f} | \"\n",
    "        f\"std: patch={patches.std():.3f} cls={cls_output.std():.3f} \"\n",
    "        f\"trans={transformed.std():.3f} quat={quat_feat.std():.3f} \"\n",
    "        f\"logit={logits.std():.3f} | \"\n",
    "        f\"dead: p={patch_dead:.2f} c={cls_dead:.2f} q={quat_dead:.2f} | \"\n",
    "        f\"div: p={patch_div:.3f} c={cls_div:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c2cebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        depth, mask, goal, quat, y = move_to_device(batch)\n",
    "        depth, mask, quat = augment_batch(depth, mask, quat)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(depth, mask, goal, quat)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item() * depth.size(0)\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val():\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    preds, targets = [], []\n",
    "    for batch in val_loader:\n",
    "        depth, mask, goal, quat, y = move_to_device(batch)\n",
    "        pred = model(depth, mask, goal, quat)\n",
    "        loss = criterion(pred, y)\n",
    "        val_loss += loss.item() * depth.size(0)\n",
    "        preds.append(pred)\n",
    "        targets.append(y)\n",
    "    return val_loss / len(val_loader.dataset), torch.cat(preds), torch.cat(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b86af088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impacts: d=0.018 m=0.001 dist=0.025 q=0.008 | std: patch=0.086 cls=1.000 trans=1.000 quat=0.198 logit=0.082 | dead: p=0.00 c=0.00 q=0.00 | div: p=0.017 c=0.019\n",
      "epoch 01: tL=0.2255 vL=0.1549 lr=1.00e-04\n",
      "impacts: d=0.040 m=0.007 dist=0.043 q=0.070 | std: patch=0.094 cls=1.002 trans=1.001 quat=0.200 logit=0.274 | dead: p=0.00 c=0.00 q=0.00 | div: p=0.019 c=0.026\n",
      "epoch 02: tL=0.1652 vL=0.1317 lr=4.74e-05\n",
      "impacts: d=0.059 m=0.025 dist=0.068 q=0.103 | std: patch=0.085 cls=1.002 trans=1.001 quat=0.200 logit=0.302 | dead: p=0.00 c=0.00 q=0.00 | div: p=0.018 c=0.059\n",
      "epoch 03: tL=0.1532 vL=0.1106 lr=1.39e-06\n",
      "impacts: d=0.089 m=0.038 dist=0.117 q=0.138 | std: patch=0.071 cls=1.002 trans=1.001 quat=0.201 logit=0.348 | dead: p=0.00 c=0.00 q=0.00 | div: p=0.018 c=0.086\n",
      "epoch 04: tL=0.1202 vL=0.0577 lr=5.98e-05\n",
      "impacts: d=0.090 m=0.053 dist=0.130 q=0.181 | std: patch=0.052 cls=1.003 trans=1.000 quat=0.201 logit=0.374 | dead: p=0.00 c=0.00 q=0.00 | div: p=0.016 c=0.146\n",
      "epoch 05: tL=0.0914 vL=0.0454 lr=9.84e-05\n",
      "impacts: d=0.068 m=0.081 dist=0.116 q=0.191 | std: patch=0.048 cls=1.003 trans=1.000 quat=0.201 logit=0.378 | dead: p=0.00 c=0.00 q=0.00 | div: p=0.016 c=0.153\n",
      "epoch 06: tL=0.0858 vL=0.0452 lr=3.52e-05\n",
      "impacts: d=0.072 m=0.076 dist=0.114 q=0.196 | std: patch=0.047 cls=1.003 trans=1.000 quat=0.201 logit=0.376 | dead: p=0.00 c=0.00 q=0.00 | div: p=0.015 c=0.149\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m):\n\u001b[32m      4\u001b[39m     lr = scheduler.get_last_lr()[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     train_loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     val_loss, pred, targets = val()\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: tL=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m vL=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m     optimizer.step()\n\u001b[32m     13\u001b[39m     scheduler.step()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * depth.size(\u001b[32m0\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss / \u001b[38;5;28mlen\u001b[39m(train_loader.dataset)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "debug_model()\n",
    "\n",
    "for epoch in range(100):\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    train_loss = train()\n",
    "    val_loss, pred, targets = val()\n",
    "\n",
    "    print(f\"epoch {epoch + 1:02d}: tL={train_loss:.4f} vL={val_loss:.4f} lr={lr:.2e}\")\n",
    "    \n",
    "    debug_model()\n",
    "    check_grad_norms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ad56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    \"epoch\": 47,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, \"checkpoint_epoch47.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
