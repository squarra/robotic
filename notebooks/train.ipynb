{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca98b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import MobileNet_V3_Small_Weights, mobilenet_v3_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66c5dd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6659a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionPoseRegressionDataset(Dataset):\n",
    "    def __init__(self, h5_path: str):\n",
    "        self.h5_path = h5_path\n",
    "        self._h5_file = None\n",
    "\n",
    "        with h5py.File(h5_path, \"r\") as f:\n",
    "            self.keys = list(f.keys())\n",
    "            self.primitives = list(f.attrs[\"primitives\"])\n",
    "\n",
    "    def _init_h5(self):\n",
    "        if self._h5_file is None:\n",
    "            self._h5_file = h5py.File(self.h5_path, \"r\")\n",
    "\n",
    "    def close(self):\n",
    "        if self._h5_file is not None:\n",
    "            self._h5_file.close()\n",
    "            self._h5_file = None\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._init_h5()\n",
    "        dp = self._h5_file[self.keys[idx]]\n",
    "        return (dp[\"depths\"][()], dp[\"masks\"][()], dp[\"goal_maps\"][()], dp[\"quat\"][()], dp[\"feasibles\"][()], dp[\"pose_diffs\"][()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8767ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VisionPoseRegressionDataset(\"../vision_pose_regression_dataset.h5\")\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "primitives = dataset.primitives\n",
    "num_primitives = len(primitives)\n",
    "print(dataset.primitives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebe556e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5943"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0470956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pos_weight() -> torch.Tensor:\n",
    "    positive_counts = torch.zeros(num_primitives)\n",
    "    for batch in DataLoader(train_dataset, batch_size=128, shuffle=False, num_workers=4):\n",
    "        positive_counts += batch[4].sum(dim=0)\n",
    "    return (len(train_dataset) - positive_counts) / (positive_counts + 1e-8)\n",
    "\n",
    "\n",
    "pos_weight = calculate_pos_weight().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89eab481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionPoseRegressionNet(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 64, head_layers: int = 2, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1\n",
    "        mnet = mobilenet_v3_small(weights=weights)\n",
    "        self.cnn = mnet.features\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        cnn_out_dim = 576\n",
    "\n",
    "        self.pose_fc = nn.Sequential(nn.Linear(4, hidden_dim), nn.ReLU())\n",
    "\n",
    "        self.view_attn = nn.Sequential(\n",
    "            nn.Linear(cnn_out_dim, cnn_out_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(cnn_out_dim // 4, 1),\n",
    "        )\n",
    "\n",
    "        mlp_input_dim = cnn_out_dim + hidden_dim\n",
    "\n",
    "        def mlp(in_dim, out_dim):\n",
    "            layers = []\n",
    "            h = hidden_dim\n",
    "            for _ in range(head_layers - 1):\n",
    "                layers += [nn.Linear(in_dim, h), nn.ReLU()]\n",
    "                if dropout > 0.0:\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "                in_dim = h\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.head_feas = mlp(mlp_input_dim, num_primitives)\n",
    "        self.head_pose = mlp(mlp_input_dim, num_primitives * 7)\n",
    "\n",
    "    def forward(self, depths, masks, goal_maps, quat):\n",
    "        B, V, H, W = depths.shape\n",
    "        per_view_feats = []\n",
    "\n",
    "        for v in range(V):\n",
    "            x_in = torch.stack([depths[:, v], masks[:, v], goal_maps[:, v]], dim=1)\n",
    "            f = self.global_pool(self.cnn(x_in)).flatten(1)\n",
    "            per_view_feats.append(f)\n",
    "\n",
    "        per_view_feats = torch.stack(per_view_feats, dim=1)\n",
    "\n",
    "        attn_logits = self.view_attn(per_view_feats)\n",
    "        attn = torch.softmax(attn_logits, dim=1)\n",
    "        x_img = (attn * per_view_feats).sum(dim=1)\n",
    "\n",
    "        x_pose = self.pose_fc(quat)\n",
    "        x = torch.cat([x_img, x_pose], dim=1)\n",
    "\n",
    "        feas_logits = self.head_feas(x)\n",
    "        pose_diff = self.head_pose(x).reshape(B, -1, 7)\n",
    "        return feas_logits, pose_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd34a9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1717633"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visionmodel = VisionPoseRegressionNet(hidden_dim=256, head_layers=4, dropout=0.125).to(device)\n",
    "optimizer = torch.optim.Adam(visionmodel.parameters(), lr=1e-3)\n",
    "bce_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "mse_loss = nn.MSELoss(reduction=\"none\")\n",
    "sum(p.numel() for p in visionmodel.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa7abefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "train_dataset_size = len(train_loader.dataset)\n",
    "test_dataset_size = len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c0dbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_LOSS_WEIGHT = 20.0\n",
    "\n",
    "def move_to_device(batch: tuple[torch.Tensor, ...]):\n",
    "    return tuple(item.to(device, non_blocking=True) for item in batch)\n",
    "\n",
    "\n",
    "def compute_masked_pose_loss(pred_pose: torch.Tensor, target_pose: torch.Tensor, feas_mask: torch.Tensor):\n",
    "    if feas_mask.sum() == 0:\n",
    "        return pred_pose.new_tensor(0.0)\n",
    "    diff = mse_loss(pred_pose, target_pose).mean(dim=-1)\n",
    "    diff = diff * feas_mask\n",
    "    return POSE_LOSS_WEIGHT * diff.sum() / (feas_mask.sum() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f67a405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    visionmodel.eval()\n",
    "    total_feas_loss, total_pose_loss, correct = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            depths, masks, goal_maps, quat, feasibles, pose_diffs = move_to_device(batch)\n",
    "\n",
    "            feas_logits, pose_pred = visionmodel(depths, masks, goal_maps, quat)\n",
    "            feas_loss = bce_loss(feas_logits, feasibles)\n",
    "            pose_loss = compute_masked_pose_loss(pose_pred, pose_diffs, feasibles)\n",
    "\n",
    "            batch_size = depths.size(0)\n",
    "            total_feas_loss += feas_loss * batch_size\n",
    "            total_pose_loss += pose_loss * batch_size\n",
    "\n",
    "            preds = torch.sigmoid(feas_logits) > 0.5\n",
    "            correct += (preds == feasibles).sum()\n",
    "\n",
    "    feas_loss = total_feas_loss / test_dataset_size\n",
    "    pose_loss = total_pose_loss / test_dataset_size\n",
    "    acc = correct / (test_dataset_size * num_primitives)\n",
    "    return feas_loss, pose_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "057c738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001: train=(feas_loss=0.9633, pose_loss=0.2448), val=(feas_loss=0.9734, pose_loss=0.2400, acc=54.97%)\n",
      "epoch 002: train=(feas_loss=0.9019, pose_loss=0.2418), val=(feas_loss=0.8646, pose_loss=0.2481, acc=59.86%)\n",
      "epoch 003: train=(feas_loss=0.7682, pose_loss=0.2377), val=(feas_loss=2.5582, pose_loss=0.2525, acc=56.52%)\n",
      "epoch 004: train=(feas_loss=0.5893, pose_loss=0.2335), val=(feas_loss=1.8807, pose_loss=0.2407, acc=62.32%)\n",
      "epoch 005: train=(feas_loss=0.5431, pose_loss=0.2305), val=(feas_loss=1.7328, pose_loss=0.2426, acc=64.88%)\n",
      "epoch 006: train=(feas_loss=0.5353, pose_loss=0.2291), val=(feas_loss=1.5449, pose_loss=0.2400, acc=66.29%)\n",
      "epoch 007: train=(feas_loss=0.5279, pose_loss=0.2290), val=(feas_loss=1.6971, pose_loss=0.2381, acc=66.33%)\n",
      "epoch 008: train=(feas_loss=0.5297, pose_loss=0.2282), val=(feas_loss=1.6933, pose_loss=0.2436, acc=64.25%)\n",
      "epoch 009: train=(feas_loss=0.5245, pose_loss=0.2274), val=(feas_loss=1.7764, pose_loss=0.2398, acc=64.17%)\n",
      "epoch 010: train=(feas_loss=0.5227, pose_loss=0.2272), val=(feas_loss=1.2450, pose_loss=0.2401, acc=66.98%)\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "EVAL_EVERY = 1\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    visionmodel.train()\n",
    "    total_feas_loss, total_pose_loss = 0.0, 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        depths, masks, goal_maps, quat, feasibles, pose_diffs = move_to_device(batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        feas_logits, pose_pred = visionmodel(depths, masks, goal_maps, quat)\n",
    "        feas_loss = bce_loss(feas_logits, feasibles)\n",
    "        pose_loss = compute_masked_pose_loss(pose_pred, pose_diffs, feasibles)\n",
    "        loss = feas_loss + pose_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = depths.size(0)\n",
    "        total_feas_loss += feas_loss * batch_size\n",
    "        total_pose_loss += pose_loss * batch_size\n",
    "\n",
    "    if (epoch + 1) % EVAL_EVERY == 0:\n",
    "        feas_loss = total_feas_loss / train_dataset_size\n",
    "        pose_loss = total_pose_loss / train_dataset_size\n",
    "        val_feas_loss, val_pose_loss, val_acc = evaluate()\n",
    "        print(\n",
    "            f\"epoch {epoch + 1:03d}: \"\n",
    "            f\"train=(feas_loss={feas_loss:.4f}, pose_loss={pose_loss:.4f}), \"\n",
    "            f\"val=(feas_loss={val_feas_loss:.4f}, pose_loss={val_pose_loss:.4f}, acc={val_acc:.2%})\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
