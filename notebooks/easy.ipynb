{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a15b5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52277178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:5\n"
     ]
    }
   ],
   "source": [
    "SOURCE_H5_PATH = \"../dataset.h5\"\n",
    "DATASET_PATH = \"../easy-dataset.h5\"\n",
    "INTRINSICS = {\"focalLength\": 1.0, \"width\": 420.0, \"height\": 360.0, \"zRange\": [0.01, 2.0]}\n",
    "\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10d37464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(batch: tuple[torch.Tensor, ...]):\n",
    "    return tuple(item.to(device, non_blocking=True) for item in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df09237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quat_to_matrix(quaternions: np.typing.ArrayLike):\n",
    "    return R.from_quat(np.roll(quaternions, -1, axis=-1)).as_matrix()\n",
    "\n",
    "\n",
    "def project_points(world_point, cam_poses):\n",
    "    cam_positions = cam_poses[:, :3]\n",
    "    cam_quaternions = cam_poses[:, 3:]\n",
    "\n",
    "    rots = quat_to_matrix(cam_quaternions)\n",
    "\n",
    "    points_rel = world_point[None, :] - cam_positions\n",
    "    p_cam = np.einsum(\"nij,nj->ni\", rots.transpose(0, 2, 1), points_rel)\n",
    "\n",
    "    f, w, h = (INTRINSICS[\"focalLength\"], INTRINSICS[\"width\"], INTRINSICS[\"height\"])\n",
    "    u = (f * p_cam[:, 0] / p_cam[:, 2]) * (w / 2) + w / 2\n",
    "    v = (f * p_cam[:, 1] / p_cam[:, 2]) * (h / 2) + h / 2\n",
    "    return np.stack([u, v], axis=1)\n",
    "\n",
    "\n",
    "def make_gaussian_maps(centers, sigma=8):\n",
    "    width, height = int(INTRINSICS[\"width\"]), int(INTRINSICS[\"height\"])\n",
    "    X, Y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    u = centers[:, 0, None, None]\n",
    "    v = centers[:, 1, None, None]\n",
    "\n",
    "    g = np.exp(-((X - u) ** 2 + (Y - v) ** 2) / (2 * sigma**2))\n",
    "    g /= g.max(axis=(1, 2), keepdims=True) + 1e-8\n",
    "    return g.astype(np.float32)\n",
    "\n",
    "\n",
    "def compute_pose_scalar_diffs(pose, final_pose):\n",
    "    pos_i, rot_i = pose[..., :3], pose[..., 3:]\n",
    "    pos_f, rot_f = final_pose[..., :3], final_pose[..., 3:]\n",
    "\n",
    "    pos_diff = np.linalg.norm(pos_f - pos_i).astype(np.float32)\n",
    "\n",
    "    rot_i = rot_i / np.linalg.norm(rot_i)\n",
    "    rot_f = rot_f / np.linalg.norm(rot_f)\n",
    "\n",
    "    dot = np.clip(np.abs(np.sum(rot_i * rot_f)), -1.0, 1.0)\n",
    "    rot_diff = (2.0 * np.arccos(dot)).astype(np.float32)\n",
    "\n",
    "    return np.array([pos_diff, rot_diff], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4aca496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scenes: 100%|██████████| 1300/1300 [01:29<00:00, 14.52it/s]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(SOURCE_H5_PATH, \"r\") as f_in, h5py.File(DATASET_PATH, \"w\") as f_out:\n",
    "    f_out.attrs.update(f_in.attrs)\n",
    "\n",
    "    for dp_key in tqdm(f_in.keys(), desc=\"Processing scenes\"):\n",
    "        dp = f_in[dp_key]\n",
    "\n",
    "        depths = dp[\"depths\"][()]\n",
    "        cam_poses = dp[\"cam_poses\"][()]\n",
    "        obj_ids = dp[\"obj_ids\"][()]\n",
    "        seg_ids = dp[\"seg_ids\"][()]\n",
    "        poses = dp[\"poses\"][()]\n",
    "        final_poses = dp[\"final_poses\"][()]\n",
    "\n",
    "        for oi in range(poses.shape[0]):\n",
    "            target_pose = dp[\"target_poses\"][oi]\n",
    "            pose_diffs = compute_pose_scalar_diffs(final_poses[oi], target_pose)\n",
    "\n",
    "            centers = project_points(target_pose[:3], cam_poses)\n",
    "            goal_maps = make_gaussian_maps(centers, sigma=6)\n",
    "\n",
    "            dp_group = f_out.create_group(f\"{dp_key}_obj_{oi}\")\n",
    "            dp_group.create_dataset(\"depths\", data=depths)\n",
    "            dp_group.create_dataset(\"masks\", data=(seg_ids == obj_ids[oi]).astype(np.float32))\n",
    "            dp_group.create_dataset(\"goal_maps\", data=goal_maps)\n",
    "            dp_group.create_dataset(\"quat\", data=poses[oi][3:])\n",
    "            dp_group.create_dataset(\"feasibles\", data=dp[\"feasibles\"][oi])\n",
    "            dp_group.create_dataset(\"pose_diffs\", data=pose_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "805b20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self._h5_file = None\n",
    "\n",
    "        with h5py.File(DATASET_PATH, \"r\") as f:\n",
    "            self.keys = list(f.keys())\n",
    "            self.primitives = list(f.attrs[\"primitives\"])\n",
    "\n",
    "    def _init_h5(self):\n",
    "        if self._h5_file is None:\n",
    "            self._h5_file = h5py.File(DATASET_PATH, \"r\")\n",
    "\n",
    "    def close(self):\n",
    "        if self._h5_file is not None:\n",
    "            self._h5_file.close()\n",
    "            self._h5_file = None\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._init_h5()\n",
    "        dp = self._h5_file[self.keys[idx]]\n",
    "        return (dp[\"depths\"][()], dp[\"masks\"][()], dp[\"goal_maps\"][()], dp[\"quat\"][()], dp[\"feasibles\"][()], dp[\"pose_diffs\"][()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c10b56c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5943\n"
     ]
    }
   ],
   "source": [
    "dataset = EasyDataset()\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "primitives = dataset.primitives\n",
    "num_primitives = len(primitives)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "589f5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, stride: int = 1, padding: int = 1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.relu(self.linear(x)))\n",
    "\n",
    "\n",
    "class EasyNet(nn.Module):\n",
    "    def __init__(self, num_views: int, num_primitives: int):\n",
    "        super().__init__()\n",
    "        self.num_views = num_views\n",
    "\n",
    "        in_channels = 3  # depth, mask, goal\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            ConvBlock(in_channels, 32, stride=2),\n",
    "            ConvBlock(32, 64, stride=1),\n",
    "            ConvBlock(64, 64, stride=2),\n",
    "            ConvBlock(64, 128, stride=1),\n",
    "            ConvBlock(128, 128, stride=2),\n",
    "            ConvBlock(128, 256, stride=1),\n",
    "        )\n",
    "\n",
    "        self.spatial_pool = nn.AdaptiveAvgPool2d((7, 6))\n",
    "\n",
    "        self.fusion_conv = nn.Sequential(\n",
    "            ConvBlock(256 * num_views, 512),\n",
    "            ConvBlock(512, 256),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.quat_encoder = nn.Sequential(\n",
    "            nn.Linear(4, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            MLPBlock(256 + 64, 512),\n",
    "            MLPBlock(512, 512),\n",
    "            MLPBlock(512, 256),\n",
    "        )\n",
    "\n",
    "        self.feasibility_head = nn.Linear(256, num_primitives)\n",
    "        self.pos_diff_head = nn.Linear(256, num_primitives)\n",
    "        self.rot_diff_head = nn.Linear(256, num_primitives)\n",
    "\n",
    "    def forward(self, depths, masks, goals, quat):\n",
    "        per_view_features = []\n",
    "        for v in range(self.num_views):\n",
    "            x = torch.stack([depths[:, v], masks[:, v], goals[:, v]], dim=1)\n",
    "            features = self.feature_extractor(x)\n",
    "            features = self.spatial_pool(features)\n",
    "            per_view_features.append(features)\n",
    "\n",
    "        multi_view = torch.cat(per_view_features, dim=1)\n",
    "        visual_features = self.fusion_conv(multi_view)\n",
    "        quat_features = self.quat_encoder(quat)\n",
    "\n",
    "        combined = torch.cat([visual_features, quat_features], dim=1)\n",
    "        features = self.mlp(combined)\n",
    "\n",
    "        feasibility_logits = self.feasibility_head(features)\n",
    "        pos_diffs = self.pos_diff_head(features)\n",
    "        rot_diffs = self.rot_diff_head(features)\n",
    "        return feasibility_logits, pos_diffs, rot_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1366581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feasibility_weights(dataset):\n",
    "    pos_counts = np.zeros(num_primitives, dtype=np.float32)\n",
    "    neg_counts = np.zeros(num_primitives, dtype=np.float32)\n",
    "\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        _, _, _, _, feasibles, _ = dataset[idx]\n",
    "        pos_counts += feasibles\n",
    "        neg_counts += 1 - feasibles\n",
    "\n",
    "    pos_weights = neg_counts / (pos_counts + 1e-8)\n",
    "\n",
    "    for i, prim in enumerate(dataset.primitives):\n",
    "        total = pos_counts[i] + neg_counts[i]\n",
    "        pos_ratio = pos_counts[i] / total\n",
    "        print(f\"{prim:15s}: {pos_ratio:.2%} feasible, pos_weight={pos_weights[i]:.3f}\")\n",
    "\n",
    "    return torch.tensor(pos_weights, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "062e8a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5943/5943 [00:16<00:00, 360.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "push_x_pos     : 16.84% feasible, pos_weight=4.937\n",
      "push_x_neg     : 17.75% feasible, pos_weight=4.633\n",
      "push_y_pos     : 17.99% feasible, pos_weight=4.559\n",
      "push_y_neg     : 16.19% feasible, pos_weight=5.178\n",
      "lift_x         : 46.47% feasible, pos_weight=1.152\n",
      "lift_y         : 47.28% feasible, pos_weight=1.115\n",
      "pull_x         : 40.57% feasible, pos_weight=1.465\n",
      "pull_y         : 40.80% feasible, pos_weight=1.451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pos_weights = compute_feasibility_weights(dataset).to(device)\n",
    "bce_loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37111f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5861944\n"
     ]
    }
   ],
   "source": [
    "model = EasyNet(num_views=3, num_primitives=num_primitives).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9af84c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01, train_loss=0.9700, val_loss=0.9660, lr=1.00e-02\n",
      "epoch 02, train_loss=0.9639, val_loss=0.9657, lr=1.00e-02\n",
      "epoch 03, train_loss=0.9636, val_loss=0.9654, lr=1.00e-02\n",
      "epoch 04, train_loss=0.9637, val_loss=0.9655, lr=1.00e-02\n",
      "epoch 05, train_loss=0.9638, val_loss=0.9656, lr=1.00e-02\n",
      "epoch 06, train_loss=0.9636, val_loss=0.9655, lr=1.00e-02\n",
      "epoch 07, train_loss=0.9638, val_loss=0.9658, lr=1.00e-02\n",
      "epoch 08, train_loss=0.9638, val_loss=0.9662, lr=1.00e-02\n",
      "epoch 09, train_loss=0.9637, val_loss=0.9663, lr=1.00e-02\n",
      "epoch 10, train_loss=0.9637, val_loss=0.9660, lr=1.00e-02\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        depths, masks, goals, quat, feasibles, _ = move_to_device(batch)\n",
    "        optimizer.zero_grad()\n",
    "        feasibility_logits, _, _ = model(depths, masks, goals, quat)\n",
    "        loss = bce_loss_fn(feasibility_logits, feasibles)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * depths.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            depths, masks, goals, quat, feasibles, _ = move_to_device(batch)\n",
    "            feasibility_logits, _, _ = model(depths, masks, goals, quat)\n",
    "            loss = bce_loss_fn(feasibility_logits, feasibles)\n",
    "            val_loss += loss.item() * depths.size(0)\n",
    "    val_loss /= len(test_dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(f\"epoch {epoch + 1:02d}, train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, lr={current_lr:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9171a05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-primitive metrics:\n",
      "Primitive       Accuracy Precision   Recall       F1\n",
      "push_x_pos        93.69%    73.38%   97.47%    0.837\n",
      "push_x_neg        91.92%    70.37%  100.00%    0.826\n",
      "push_y_pos        90.91%    65.59%   99.51%    0.791\n",
      "push_y_neg        91.67%    65.98%  100.00%    0.795\n",
      "lift_x            56.23%    54.09%   48.31%    0.510\n",
      "lift_y            54.46%    51.45%   84.15%    0.639\n",
      "pull_x            58.16%    49.91%   54.23%    0.520\n",
      "pull_y            53.03%    46.75%   77.18%    0.582\n",
      "Overall           73.76%    55.74%   75.34%    0.641\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "tp = np.zeros(num_primitives)\n",
    "fp = np.zeros(num_primitives)\n",
    "tn = np.zeros(num_primitives)\n",
    "fn = np.zeros(num_primitives)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        depths, masks, goals, quat, feasibles, _ = move_to_device(batch)\n",
    "        feasibility_logits, _, _ = model(depths, masks, goals, quat)\n",
    "        preds = (torch.sigmoid(feasibility_logits) > 0.5).float()\n",
    "        feasibles_np = feasibles.cpu().numpy()\n",
    "        preds_np = preds.cpu().numpy()\n",
    "\n",
    "        tp += ((preds_np == 1) & (feasibles_np == 1)).sum(axis=0)\n",
    "        fp += ((preds_np == 1) & (feasibles_np == 0)).sum(axis=0)\n",
    "        tn += ((preds_np == 0) & (feasibles_np == 0)).sum(axis=0)\n",
    "        fn += ((preds_np == 0) & (feasibles_np == 1)).sum(axis=0)\n",
    "\n",
    "print(\"Per-primitive metrics:\")\n",
    "print(f\"{'Primitive':<15} {'Accuracy':>8} {'Precision':>9} {'Recall':>8} {'F1':>8}\")\n",
    "\n",
    "for i, prim in enumerate(primitives):\n",
    "    accuracy = (tp[i] + tn[i]) / (tp[i] + tn[i] + fp[i] + fn[i])\n",
    "    precision = tp[i] / (tp[i] + fp[i]) if (tp[i] + fp[i]) > 0 else 0.0\n",
    "    recall = tp[i] / (tp[i] + fn[i]) if (tp[i] + fn[i]) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    print(f\"{prim:<15} {accuracy:>8.2%} {precision:>9.2%} {recall:>8.2%} {f1:>8.3f}\")\n",
    "\n",
    "overall_accuracy = (tp.sum() + tn.sum()) / (tp.sum() + tn.sum() + fp.sum() + fn.sum())\n",
    "overall_precision = tp.sum() / (tp.sum() + fp.sum())\n",
    "overall_recall = tp.sum() / (tp.sum() + fn.sum())\n",
    "overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall)\n",
    "\n",
    "print(f\"{'Overall':<15} {overall_accuracy:>8.2%} {overall_precision:>9.2%} {overall_recall:>8.2%} {overall_f1:>8.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
