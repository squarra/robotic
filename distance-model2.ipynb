{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e64552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d986d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"combined-dataset.h5\"\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c4b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(batch):\n",
    "    return tuple(item.to(DEVICE, non_blocking=True) for item in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8ddc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_batch(depth, mask, quat):\n",
    "    batch_size = depth.shape[0]\n",
    "\n",
    "    # Gaussian noise\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        noise = torch.randn_like(depth) * 0.02\n",
    "        depth = depth + noise\n",
    "\n",
    "    # Scale variations\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        scales = torch.empty(batch_size, 1, 1, device=DEVICE).uniform_(0.95, 1.05)\n",
    "        depth = depth * scales\n",
    "\n",
    "    # Small per-sample offset\n",
    "    if torch.rand(1).item() < 0.3:\n",
    "        offsets = torch.empty(batch_size, 1, 1, device=DEVICE).uniform_(-0.03, 0.03)\n",
    "        depth = depth + offsets\n",
    "\n",
    "    depth = torch.clamp(depth, 0, 1)\n",
    "\n",
    "    # Mask augmentation\n",
    "    if torch.rand(1).item() < 0.4:\n",
    "        mask_4d = mask.unsqueeze(1)\n",
    "        if torch.rand(1).item() < 0.8:\n",
    "            mask_4d = F.max_pool2d(mask_4d, 3, stride=1, padding=1)\n",
    "        else:\n",
    "            mask_4d = -F.max_pool2d(-mask_4d, 3, stride=1, padding=1)\n",
    "        mask = mask_4d.squeeze(1)\n",
    "\n",
    "    # Quaternion augmentation\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        angles = torch.empty(batch_size, device=DEVICE).uniform_(-0.175, 0.175)\n",
    "        half_angles = angles / 2\n",
    "\n",
    "        perturb_quats = torch.stack(\n",
    "            [torch.cos(half_angles), torch.zeros_like(half_angles), torch.zeros_like(half_angles), torch.sin(half_angles)], dim=1\n",
    "        )\n",
    "\n",
    "        w1, x1, y1, z1 = perturb_quats.unbind(1)\n",
    "        w2, x2, y2, z2 = quat.unbind(1)\n",
    "\n",
    "        quat = torch.stack(\n",
    "            [\n",
    "                w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2,\n",
    "                w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2,\n",
    "                w1 * y2 - x1 * z2 + y1 * w2 + z1 * x2,\n",
    "                w1 * z2 + x1 * y2 - y1 * x2 + z1 * w2,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        quat = F.normalize(quat, dim=1)\n",
    "\n",
    "    return depth, mask, quat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d902996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self._h5_file = None\n",
    "        with h5py.File(DATASET_PATH, \"r\") as f:\n",
    "            self.keys = list(f.keys())\n",
    "            self.primitives = list(f.attrs[\"primitives\"])\n",
    "            self.depth_min = f.attrs[\"depth_min\"]\n",
    "            self.depth_max = f.attrs[\"depth_max\"]\n",
    "            self.dist_max = f.attrs[\"dist_max\"]\n",
    "\n",
    "    def _init_h5(self):\n",
    "        if self._h5_file is None:\n",
    "            self._h5_file = h5py.File(DATASET_PATH, \"r\")\n",
    "\n",
    "    def close(self):\n",
    "        if self._h5_file is not None:\n",
    "            self._h5_file.close()\n",
    "            self._h5_file = None\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._init_h5()\n",
    "        dp = self._h5_file[self.keys[idx]]\n",
    "        return dp[\"depth\"][()], dp[\"mask\"][()], dp[\"dist_map\"][()], dp[\"quat\"][()], dp[\"feasibles\"][()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea0f804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total scenes: 10996\n",
      "Train: 8796 scenes, 39784 samples\n",
      "Val: 1099 scenes, 4915 samples\n",
      "Test: 1101 scenes, 4995 samples\n",
      "Sample shapes: [(256, 384), (256, 384), (256, 384), (4,), (8,)]\n"
     ]
    }
   ],
   "source": [
    "dataset = DistanceDataset()\n",
    "primitives = dataset.primitives\n",
    "num_primitives = len(primitives)\n",
    "\n",
    "scene_to_indices = defaultdict(list)\n",
    "for idx, key in enumerate(dataset.keys):\n",
    "    scene_to_indices[key.rsplit(\"_obj_\", 1)[0]].append(idx)\n",
    "scene_ids = list(scene_to_indices.keys())\n",
    "\n",
    "\n",
    "num_train = int(len(scene_ids) * 0.8)\n",
    "num_val = int(len(scene_ids) * 0.1)\n",
    "train_scenes = scene_ids[:num_train]\n",
    "val_scenes = scene_ids[num_train : num_train + num_val]\n",
    "test_scenes = scene_ids[num_train + num_val :]\n",
    "\n",
    "train_indices = [idx for scene in train_scenes for idx in scene_to_indices[scene]]\n",
    "val_indices = [idx for scene in val_scenes for idx in scene_to_indices[scene]]\n",
    "test_indices = [idx for scene in test_scenes for idx in scene_to_indices[scene]]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Total scenes: {len(scene_ids)}\")\n",
    "print(f\"Train: {len(train_scenes)} scenes, {len(train_indices)} samples\")\n",
    "print(f\"Val: {len(val_scenes)} scenes, {len(val_indices)} samples\")\n",
    "print(f\"Test: {len(test_scenes)} scenes, {len(test_indices)} samples\")\n",
    "print(f\"Sample shapes: {[item.shape for item in dataset[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f2f1edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: tensor([0.8307, 0.8268, 0.8255, 0.8309, 0.5378, 0.5309, 0.5987, 0.5940],\n",
      "       device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "pos_counts = torch.zeros(num_primitives)\n",
    "neg_counts = torch.zeros(num_primitives)\n",
    "with h5py.File(DATASET_PATH, \"r\") as f:\n",
    "    for key in dataset.keys:\n",
    "        feasibles = torch.from_numpy(f[key][\"feasibles\"][()])\n",
    "        pos_counts += feasibles\n",
    "        neg_counts += 1 - feasibles\n",
    "alpha = neg_counts / (pos_counts + neg_counts)\n",
    "alpha = alpha.to(DEVICE)\n",
    "print(f\"Alpha: {alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a4c74ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "        probs = torch.sigmoid(logits)\n",
    "        pt = torch.where(targets == 1, probs, 1 - probs)\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "        alpha_weight = torch.where(targets == 1, self.alpha, 1 - self.alpha)\n",
    "        loss = alpha_weight * focal_weight * bce_loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff501522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(probs: torch.Tensor, targets: torch.Tensor, threshold=0.5):\n",
    "    preds = (probs > threshold).float()\n",
    "    accuracy = (preds == targets).float().mean(dim=0)\n",
    "\n",
    "    tp = (preds * targets).sum(dim=0)\n",
    "    fp = (preds * (1 - targets)).sum(dim=0)\n",
    "    fn = ((1 - preds) * targets).sum(dim=0)\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    prob_mean = probs.mean(dim=0)\n",
    "    prob_std = probs.std(dim=0)\n",
    "\n",
    "    return accuracy, precision, recall, f1, prob_mean, prob_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57b26fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=(256, 384), patch_size=16, in_channels=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size, img_size[1] // patch_size)\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        nn.init.trunc_normal_(self.proj.weight, std=0.02)\n",
    "        nn.init.zeros_(self.proj.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size=(256, 384), patch_size=16, in_channels=3, embed_dim=192, depth=4, num_heads=6, mlp_ratio=2.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=False,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        self._init_transformer_weights()\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def _init_transformer_weights(self):\n",
    "        for module in self.transformer.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.trunc_normal_(module.weight, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "        x = x[:, 0]\n",
    "        return x\n",
    "\n",
    "\n",
    "class DistanceViT(nn.Module):\n",
    "    def __init__(self, img_size=(256, 384), patch_size=16, embed_dim=256, depth=6, num_heads=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vision_encoder = ViT(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=3,\n",
    "            embed_dim=embed_dim,\n",
    "            depth=depth,\n",
    "            num_heads=num_heads,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "\n",
    "        self.quat_encoder = nn.Sequential(\n",
    "            nn.Linear(4, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim + 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_primitives),\n",
    "        )\n",
    "\n",
    "    def forward(self, depth, mask, goal, quat):\n",
    "        vision_feat = self.vision_encoder(torch.stack([depth, mask, goal], dim=1))\n",
    "        quat_feat = self.quat_encoder(quat)\n",
    "        return self.classifier(torch.cat([vision_feat, quat_feat], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3a9758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 3,756,872\n"
     ]
    }
   ],
   "source": [
    "model = DistanceViT().to(DEVICE)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "criterion = FocalLoss(alpha=alpha, gamma=2.0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=2e-4, epochs=100, steps_per_epoch=len(train_loader), pct_start=0.2, anneal_strategy=\"cos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b8bf237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grad_norms():\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            if grad_norm < 1e-7 or grad_norm > 5e-1:\n",
    "                print(f\"{name:50s} | grad_norm: {grad_norm:.2e}\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def debug_model():\n",
    "    model.eval()\n",
    "    batch = next(iter(train_loader))\n",
    "    depth, mask, dist_map, quat, _ = move_to_device(batch)\n",
    "\n",
    "    vision_input = torch.stack([depth, mask, dist_map], dim=1)\n",
    "    B = vision_input.shape[0]\n",
    "\n",
    "    patches = model.vision_encoder.patch_embed(vision_input)\n",
    "    cls_tokens = model.vision_encoder.cls_token.expand(B, -1, -1)\n",
    "    patches_with_cls = torch.cat([cls_tokens, patches], dim=1)\n",
    "    patches_pos = patches_with_cls + model.vision_encoder.pos_embed\n",
    "    transformed = model.vision_encoder.transformer(patches_pos)\n",
    "    cls_output = transformed[:, 0]\n",
    "\n",
    "    quat_feat = model.quat_encoder(quat)\n",
    "    logits = model(depth, mask, dist_map, quat)\n",
    "\n",
    "    depth_impact = (logits - model(torch.zeros_like(depth), mask, dist_map, quat)).abs().mean()\n",
    "    mask_impact = (logits - model(depth, torch.zeros_like(mask), dist_map, quat)).abs().mean()\n",
    "    dist_impact = (logits - model(depth, mask, torch.zeros_like(dist_map), quat)).abs().mean()\n",
    "    quat_impact = (logits - model(depth, mask, dist_map, torch.zeros_like(quat))).abs().mean()\n",
    "\n",
    "    patch_dead = (patches.abs() < 1e-5).float().mean()\n",
    "    cls_dead = (cls_output.abs() < 1e-5).float().mean()\n",
    "    quat_dead = (quat_feat.abs() < 1e-5).float().mean()\n",
    "\n",
    "    patch_div = patches.std(dim=0).mean()\n",
    "    cls_div = cls_output.std(dim=0).mean()\n",
    "\n",
    "    print(\n",
    "        f\"impacts: d={depth_impact:.3f} m={mask_impact:.3f} \"\n",
    "        f\"dist={dist_impact:.3f} q={quat_impact:.3f} | \"\n",
    "        f\"std: patch={patches.std():.3f} cls={cls_output.std():.3f} \"\n",
    "        f\"trans={transformed.std():.3f} quat={quat_feat.std():.3f} \"\n",
    "        f\"logit={logits.std():.3f} | \"\n",
    "        f\"dead: p={patch_dead:.2f} c={cls_dead:.2f} q={quat_dead:.2f} | \"\n",
    "        f\"div: p={patch_div:.3f} c={cls_div:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c2cebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        depth, mask, goal, quat, y = move_to_device(batch)\n",
    "        depth, mask, quat = augment_batch(depth, mask, quat)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(depth, mask, goal, quat)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item() * depth.size(0)\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val():\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    preds, targets = [], []\n",
    "    for batch in val_loader:\n",
    "        depth, mask, goal, quat, y = move_to_device(batch)\n",
    "        pred = model(depth, mask, goal, quat)\n",
    "        loss = criterion(pred, y)\n",
    "        val_loss += loss.item() * depth.size(0)\n",
    "        preds.append(pred)\n",
    "        targets.append(y)\n",
    "    return val_loss / len(val_loader.dataset), torch.cat(preds), torch.cat(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86af088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impacts: d=0.049 m=0.001 dist=0.026 q=0.005 | std: patch=0.189 cls=1.000 trans=1.000 quat=0.205 logit=0.082 | dead: p=0.00 c=0.00 q=0.00 | div: p=0.036 c=0.035\n",
      "epoch 01: tL=0.0673 vL=0.0670 lr=8.00e-06\n",
      "  push_x_pos   A=0.837 P=0.000 R=0.000 F1=0.000 M=0.492 S=0.001\n",
      "  push_x_neg   A=0.176 P=0.176 R=1.000 F1=0.300 M=0.502 S=0.001\n",
      "  push_y_pos   A=0.816 P=0.156 R=0.012 F1=0.022 M=0.498 S=0.001\n",
      "  push_y_neg   A=0.799 P=0.140 R=0.040 F1=0.063 M=0.497 S=0.003\n",
      "  lift_x       A=0.538 P=0.000 R=0.000 F1=0.000 M=0.496 S=0.002\n",
      "  lift_y       A=0.557 P=0.683 R=0.132 F1=0.222 M=0.499 S=0.000\n",
      "  pull_x       A=0.604 P=0.000 R=0.000 F1=0.000 M=0.495 S=0.000\n",
      "  pull_y       A=0.447 P=0.422 R=0.893 F1=0.573 M=0.501 S=0.001\n",
      "impacts: d=0.023 m=0.001 dist=0.029 q=0.004 | std: patch=0.189 cls=1.000 trans=1.000 quat=0.205 logit=0.013 | dead: p=0.00 c=0.00 q=0.00 | div: p=0.035 c=0.029\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0.0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "debug_model()\n",
    "\n",
    "for epoch in range(100):\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    train_loss = train()\n",
    "    val_loss, logits, targets = val()\n",
    "\n",
    "    print(f\"epoch {epoch + 1:02d}: tL={train_loss:.4f} vL={val_loss:.4f} lr={lr:.2e}\")\n",
    "\n",
    "    acc, prec, rec, f1, prob_mean, prob_std = compute_metrics(logits.sigmoid(), targets)\n",
    "    for i, primitive in enumerate(primitives):\n",
    "        print(f\"  {primitive:<12} A={acc[i]:.3f} P={prec[i]:.3f} R={rec[i]:.3f} F1={f1[i]:.3f} M={prob_mean[i]:.3f} S={prob_std[i]:.3f}\")\n",
    "\n",
    "    avg_f1 = f1.mean().item()\n",
    "    if avg_f1 > best_f1:\n",
    "        best_f1 = avg_f1\n",
    "        patience_counter = 0\n",
    "        best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                \"val_loss\": val_loss,\n",
    "                \"avg_f1\": avg_f1,\n",
    "            }\n",
    "\n",
    "            torch.save(checkpoint, f\"distance-model-{epoch}.pt\")\n",
    "            patience_counter = 0\n",
    "\n",
    "    debug_model()\n",
    "    check_grad_norms()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
